{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrew/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from board import move\n",
    "from board import create_action_mask\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "import sys\n",
    "from model import BoardGFLowNet\n",
    "from board import random_board, get_reward, move, create_action_mask\n",
    "import wandb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sample_move(boards: torch.Tensor, \n",
    "                logits: torch.Tensor, \n",
    "                at_step_limit: bool):\n",
    "    \n",
    "    batch_size, _, _ = boards.shape\n",
    "    last_logits = logits[:, -1, :]\n",
    "    \n",
    "    if(at_step_limit):\n",
    "        mask = torch.ones(6) * -1e20\n",
    "        mask[1] = 0\n",
    "        mask = mask.expand((batch_size, 6))\n",
    "    else:\n",
    "        mask = create_action_mask(boards)\n",
    "    \n",
    "    last_logits = torch.softmax(mask + last_logits, dim=1)\n",
    "    new_moves = Categorical(probs=last_logits).sample()\n",
    "    new_moves = torch.Tensor(new_moves).type(torch.LongTensor)\n",
    "    return new_moves, last_logits[torch.arange(batch_size), new_moves]\n",
    "\n",
    "def loss_fn(predicted_logZ: torch.Tensor, \n",
    "            reward: torch.Tensor, \n",
    "            forward_probabilities: torch.Tensor):\n",
    "    \n",
    "    log_Pf = torch.log(forward_probabilities).sum(dim=1)\n",
    "    inner = predicted_logZ.squeeze() + log_Pf - torch.log(reward) \n",
    "    return inner ** 2\n",
    "\n",
    "def train(\n",
    "    lr=1e-4,\n",
    "    decoder_layers=3,\n",
    "    encoder_layers=3,\n",
    "    embed_dim=32,\n",
    "    d_ff=32,\n",
    "    n_heads=8,\n",
    "    batch_size=16,\n",
    "    side_len=3,\n",
    "    max_steps=20,\n",
    "    total_batches=1000,\n",
    "    checkpoint_freq=10,\n",
    "    ):\n",
    "    \n",
    "    gfn = BoardGFLowNet(side_len, embed_dim, d_ff, n_heads, encoder_layers, decoder_layers, 6)\n",
    "    optimizer = torch.optim.Adam(gfn.parameters(), lr=lr)\n",
    "\n",
    "    for batch in range(total_batches):\n",
    "    \n",
    "        boards = random_board(batch_size, side_len) \n",
    "        finished = torch.zeros((batch_size, 1))\n",
    "        moves = torch.zeros(batch_size, 1).type(torch.LongTensor)\n",
    "        forward_probabilities = torch.ones(batch_size, 1)\n",
    "\n",
    "        predicted_logZ, _ = gfn(boards, moves)\n",
    "        batch_loss = 0\n",
    "        batch_reward = 0\n",
    "        batch_matching = 0\n",
    "        \n",
    "        for i in range(max_steps):\n",
    "            _, logits = gfn(boards, moves)\n",
    "            new_move, move_prob = sample_move(boards, logits, i == max_steps-1)\n",
    "\n",
    "            for index in range(len(move_prob)):\n",
    "                if(finished[index] == 1):\n",
    "                    move_prob[index] = 1\n",
    "            for index, _move in enumerate(new_move):\n",
    "                if(_move == 1):\n",
    "                    finished[index] = 1\n",
    "\n",
    "            forward_probabilities = torch.cat([forward_probabilities, move_prob.unsqueeze(1)], dim=1)\n",
    "            moves = torch.cat([moves, new_move.unsqueeze(1)], dim=1)\n",
    "            boards = boards.clone()\n",
    "            boards = move(boards, new_move, finished_mask=finished)\n",
    "        \n",
    "        reward, matching = get_reward(boards)\n",
    "        loss = loss_fn(predicted_logZ, reward, forward_probabilities)\n",
    "        loss = torch.sum(loss)\n",
    "        reward = torch.sum(reward)\n",
    "        matching = torch.sum(matching)\n",
    "        loss.backward(retain_graph=False)\n",
    "        batch_reward += reward\n",
    "        batch_matching += matching\n",
    "        batch_loss += loss\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        batch_reward = batch_reward.item() / batch_size\n",
    "        batch_matching = batch_matching.item() / batch_size\n",
    "        batch_loss = batch_loss.item() / batch_size\n",
    "        print(f'Batch {batch}, loss: {batch_loss}, reward: {batch_reward}, Matching: {batch_matching}')\n",
    "        if((batch+1) % checkpoint_freq == 0):\n",
    "            torch.save(gfn.state_dict(), f'checkpoints/model_step_{batch}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, loss: 37.42610549926758, reward: 0.0006950950482860208, Matching: 1.4375\n",
      "Batch 1, loss: 42.69380187988281, reward: 0.001965435454621911, Matching: 1.6875\n",
      "Batch 2, loss: 57.75373077392578, reward: 0.0008981323335319757, Matching: 1.125\n",
      "Batch 3, loss: 77.88799285888672, reward: 0.0010476823663339019, Matching: 1.6875\n",
      "Batch 4, loss: 65.56709289550781, reward: 0.0007311212830245495, Matching: 1.5\n",
      "Batch 5, loss: 59.6051139831543, reward: 0.0017600570572540164, Matching: 1.625\n",
      "Batch 6, loss: 75.13094329833984, reward: 0.0018536229617893696, Matching: 1.375\n",
      "Batch 7, loss: 102.02838897705078, reward: 0.003063319716602564, Matching: 2.0\n",
      "Batch 8, loss: 62.86403274536133, reward: 0.0008039365638978779, Matching: 1.1875\n",
      "Batch 9, loss: 85.043701171875, reward: 0.0010099445935338736, Matching: 1.4375\n",
      "Batch 10, loss: 73.5926742553711, reward: 0.0043679517693817616, Matching: 1.8125\n",
      "Batch 11, loss: 56.23155975341797, reward: 0.0015761922113597393, Matching: 1.1875\n",
      "Batch 12, loss: 77.60367584228516, reward: 0.000947411812376231, Matching: 1.25\n",
      "Batch 13, loss: 86.43485260009766, reward: 0.0014359768247231841, Matching: 1.5625\n",
      "Batch 14, loss: 80.38095092773438, reward: 0.002068017143756151, Matching: 1.9375\n",
      "Batch 15, loss: 87.08732604980469, reward: 0.0031949339900165796, Matching: 1.75\n",
      "Batch 16, loss: 77.63539123535156, reward: 0.0018197748577222228, Matching: 1.75\n",
      "Batch 17, loss: 81.30319213867188, reward: 0.0006553351995535195, Matching: 1.25\n",
      "Batch 18, loss: 62.98802185058594, reward: 0.0012064330512657762, Matching: 1.75\n",
      "Batch 19, loss: 111.63609313964844, reward: 0.0012622612994164228, Matching: 1.25\n",
      "Batch 20, loss: 89.66878509521484, reward: 0.000717238406650722, Matching: 1.25\n",
      "Batch 21, loss: 85.97029113769531, reward: 0.0008644473273307085, Matching: 1.4375\n",
      "Batch 22, loss: 118.21642303466797, reward: 0.004198599606752396, Matching: 1.8125\n",
      "Batch 23, loss: 85.0445556640625, reward: 0.06330574303865433, Matching: 1.6875\n",
      "Batch 24, loss: 76.3399429321289, reward: 0.004670177586376667, Matching: 1.9375\n",
      "Batch 25, loss: 96.71549987792969, reward: 0.0017759405309334397, Matching: 1.9375\n",
      "Batch 26, loss: 90.0200424194336, reward: 0.004136695992201567, Matching: 1.8125\n",
      "Batch 27, loss: 70.33406066894531, reward: 0.0006193089066073298, Matching: 1.1875\n",
      "Batch 28, loss: 74.70836639404297, reward: 0.004719456657767296, Matching: 2.0625\n",
      "Batch 29, loss: 79.67314147949219, reward: 0.0006325622089207172, Matching: 1.25\n",
      "Batch 30, loss: 87.76075744628906, reward: 0.0022230343893170357, Matching: 1.875\n",
      "Batch 31, loss: 89.48201751708984, reward: 0.005110722500830889, Matching: 2.5\n",
      "Batch 32, loss: 62.97136306762695, reward: 0.0012982876505702734, Matching: 1.3125\n",
      "Batch 33, loss: 72.38777160644531, reward: 0.0023278528824448586, Matching: 1.625\n",
      "Batch 34, loss: 71.97738647460938, reward: 0.0036998854484409094, Matching: 1.375\n",
      "Batch 35, loss: 80.43035888671875, reward: 0.0016583938850089908, Matching: 1.4375\n",
      "Batch 36, loss: 69.59015655517578, reward: 0.005651093553751707, Matching: 2.3125\n",
      "Batch 37, loss: 44.90829086303711, reward: 0.004940111190080643, Matching: 2.125\n",
      "Batch 38, loss: 58.28626251220703, reward: 0.0013037328608334064, Matching: 1.625\n",
      "Batch 39, loss: 55.138816833496094, reward: 0.0017954536015167832, Matching: 1.5\n",
      "Batch 40, loss: 58.64472961425781, reward: 0.0010681140702217817, Matching: 1.3125\n",
      "Batch 41, loss: 70.74310302734375, reward: 0.002559737768024206, Matching: 1.8125\n",
      "Batch 42, loss: 49.27383804321289, reward: 0.0011204977054148912, Matching: 1.375\n",
      "Batch 43, loss: 56.25019454956055, reward: 0.0031738723628222942, Matching: 1.9375\n",
      "Batch 44, loss: 57.29782485961914, reward: 0.0008909538737498224, Matching: 1.5625\n",
      "Batch 45, loss: 49.36799240112305, reward: 0.004743622615933418, Matching: 1.8125\n",
      "Batch 46, loss: 56.36620330810547, reward: 0.0011211273958906531, Matching: 1.5625\n",
      "Batch 47, loss: 51.07272720336914, reward: 0.00047520414227619767, Matching: 0.9375\n",
      "Batch 48, loss: 48.96052932739258, reward: 0.0012496376875787973, Matching: 1.375\n",
      "Batch 49, loss: 47.142276763916016, reward: 0.0004625804431270808, Matching: 1.0625\n",
      "Batch 50, loss: 45.15309524536133, reward: 0.001081367488950491, Matching: 1.375\n",
      "Batch 51, loss: 48.99748992919922, reward: 0.0029694128315895796, Matching: 1.9375\n",
      "Batch 52, loss: 40.40766143798828, reward: 0.0015831857454031706, Matching: 1.75\n",
      "Batch 53, loss: 47.478248596191406, reward: 0.004399903118610382, Matching: 2.25\n",
      "Batch 54, loss: 36.712921142578125, reward: 0.00048472374328412116, Matching: 0.875\n",
      "Batch 55, loss: 49.95252990722656, reward: 0.06346619129180908, Matching: 1.9375\n",
      "Batch 56, loss: 35.12452697753906, reward: 0.0015345358988270164, Matching: 1.8125\n",
      "Batch 57, loss: 46.86534881591797, reward: 0.005399065092206001, Matching: 2.375\n",
      "Batch 58, loss: 31.490297317504883, reward: 0.0015699324430897832, Matching: 1.6875\n",
      "Batch 59, loss: 23.503572463989258, reward: 0.0010453411377966404, Matching: 1.3125\n",
      "Batch 60, loss: 29.480146408081055, reward: 0.0009871716611087322, Matching: 1.4375\n",
      "Batch 61, loss: 27.947311401367188, reward: 0.0012190567795187235, Matching: 1.625\n",
      "Batch 62, loss: 22.525842666625977, reward: 0.0007244168664328754, Matching: 0.8125\n",
      "Batch 63, loss: 17.576961517333984, reward: 0.0010946207912638783, Matching: 1.4375\n",
      "Batch 64, loss: 21.318994522094727, reward: 0.0010681141866371036, Matching: 1.3125\n",
      "Batch 65, loss: 21.94009780883789, reward: 0.0008134562522172928, Matching: 1.125\n",
      "Batch 66, loss: 17.619556427001953, reward: 0.0008892422774806619, Matching: 1.375\n",
      "Batch 67, loss: 19.086315155029297, reward: 0.0005642435280606151, Matching: 1.25\n",
      "Batch 68, loss: 25.005970001220703, reward: 0.0006356663070619106, Matching: 1.1875\n",
      "Batch 69, loss: 21.396390914916992, reward: 0.001916155801154673, Matching: 1.5625\n",
      "Batch 70, loss: 25.322223663330078, reward: 0.000877700571436435, Matching: 1.5\n",
      "Batch 71, loss: 20.667781829833984, reward: 0.0013043624348938465, Matching: 1.8125\n",
      "Batch 72, loss: 13.277812957763672, reward: 0.0004398075398057699, Matching: 1.0625\n",
      "Batch 73, loss: 14.680654525756836, reward: 0.013168488629162312, Matching: 2.3125\n",
      "Batch 74, loss: 9.764766693115234, reward: 0.0007039851043373346, Matching: 1.1875\n",
      "Batch 75, loss: 12.72465991973877, reward: 0.0013530122814700007, Matching: 1.75\n",
      "Batch 76, loss: 20.837844848632812, reward: 0.0017664209008216858, Matching: 2.0\n",
      "Batch 77, loss: 16.383708953857422, reward: 0.001072477432899177, Matching: 1.625\n",
      "Batch 78, loss: 9.689396858215332, reward: 0.0007443745853379369, Matching: 1.5625\n",
      "Batch 79, loss: 15.809732437133789, reward: 0.0012058033607900143, Matching: 1.5625\n",
      "Batch 80, loss: 8.518843650817871, reward: 0.0014328728429973125, Matching: 1.625\n",
      "Batch 81, loss: 15.064546585083008, reward: 0.0018228788394480944, Matching: 1.6875\n",
      "Batch 82, loss: 17.093080520629883, reward: 0.004002740606665611, Matching: 1.6875\n",
      "Batch 83, loss: 21.400728225708008, reward: 0.0013740736758336425, Matching: 1.5625\n",
      "Batch 84, loss: 19.256595611572266, reward: 0.0012064330512657762, Matching: 1.75\n",
      "Batch 85, loss: 10.060445785522461, reward: 0.004473689012229443, Matching: 1.625\n",
      "Batch 86, loss: 14.451396942138672, reward: 0.0013302394654601812, Matching: 1.75\n",
      "Batch 87, loss: 18.523902893066406, reward: 0.00048535337555222213, Matching: 1.0625\n",
      "Batch 88, loss: 16.076061248779297, reward: 0.0008987619075924158, Matching: 1.3125\n",
      "Batch 89, loss: 12.353553771972656, reward: 0.00510544003918767, Matching: 2.125\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m l \u001b[39m=\u001b[39m train()\n",
      "Cell \u001b[0;32mIn[56], line 93\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(lr, decoder_layers, encoder_layers, embed_dim, d_ff, n_heads, batch_size, side_len, max_steps, total_batches, checkpoint_freq)\u001b[0m\n\u001b[1;32m     90\u001b[0m reward \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msum(reward)\n\u001b[1;32m     91\u001b[0m matching \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msum(matching)\n\u001b[0;32m---> 93\u001b[0m loss\u001b[39m.\u001b[39;49mbackward(retain_graph\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     95\u001b[0m batch_reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward\n\u001b[1;32m     96\u001b[0m batch_matching \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m matching\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "l = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward(boards: torch.Tensor):\n",
    "    batch_size, _, side_len = boards.shape\n",
    "    ground_truth = torch.arange(0, side_len**2).reshape(side_len,side_len).expand_as(boards)\n",
    "    mismatch = boards - ground_truth\n",
    "    \n",
    "    match = mismatch == 0\n",
    "    mismatch = mismatch != 0\n",
    "    num_mismatch = mismatch.flatten(1).count_nonzero(1)\n",
    "    num_match = match.flatten(1).count_nonzero(1)\n",
    "    reward = torch.exp(-num_mismatch) \n",
    "    return reward, num_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0009, 0.0009, 0.0009, 0.0003, 0.0001, 0.0001, 0.0003, 0.0001, 0.0025,\n",
       "         0.0025, 0.0003, 0.0003, 0.0001, 0.0001, 0.0001, 0.0009]),\n",
       " tensor([2, 2, 2, 1, 0, 0, 1, 0, 3, 3, 1, 1, 0, 0, 0, 2]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_reward(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "boards = random_board(100, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  4,  5,  1],\n",
       "        [ 6,  2, 11,  3],\n",
       "        [ 8, 14, 13,  7],\n",
       "        [ 9, 12, 10, 15]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boards[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11],\n",
       "         [12, 13, 14, 15]],\n",
       "\n",
       "        [[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11],\n",
       "         [12, 13, 14, 15]],\n",
       "\n",
       "        [[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11],\n",
       "         [12, 13, 14, 15]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11],\n",
       "         [12, 13, 14, 15]],\n",
       "\n",
       "        [[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11],\n",
       "         [12, 13, 14, 15]],\n",
       "\n",
       "        [[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11],\n",
       "         [12, 13, 14, 15]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(16).reshape(4,4).expand(100, 4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_UP = 2\n",
    "DIR_DOWN = 3\n",
    "DIR_RIGHT = 4\n",
    "DIR_LEFT = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[2, 4, 3],\n",
       "          [8, 0, 1],\n",
       "          [6, 7, 5]],\n",
       " \n",
       "         [[0, 3, 2],\n",
       "          [7, 5, 8],\n",
       "          [6, 1, 4]],\n",
       " \n",
       "         [[4, 7, 3],\n",
       "          [0, 8, 5],\n",
       "          [2, 6, 1]],\n",
       " \n",
       "         [[5, 6, 1],\n",
       "          [8, 3, 2],\n",
       "          [7, 0, 4]],\n",
       " \n",
       "         [[5, 3, 1],\n",
       "          [6, 7, 0],\n",
       "          [2, 4, 8]],\n",
       " \n",
       "         [[3, 7, 2],\n",
       "          [6, 1, 8],\n",
       "          [5, 4, 0]],\n",
       " \n",
       "         [[1, 4, 3],\n",
       "          [8, 5, 0],\n",
       "          [6, 7, 2]],\n",
       " \n",
       "         [[4, 2, 7],\n",
       "          [8, 6, 3],\n",
       "          [0, 1, 5]],\n",
       " \n",
       "         [[1, 2, 5],\n",
       "          [0, 3, 7],\n",
       "          [4, 6, 8]],\n",
       " \n",
       "         [[3, 0, 1],\n",
       "          [5, 4, 2],\n",
       "          [6, 8, 7]],\n",
       " \n",
       "         [[4, 5, 1],\n",
       "          [2, 3, 0],\n",
       "          [6, 7, 8]],\n",
       " \n",
       "         [[3, 1, 4],\n",
       "          [6, 5, 7],\n",
       "          [0, 8, 2]],\n",
       " \n",
       "         [[6, 0, 2],\n",
       "          [5, 1, 8],\n",
       "          [4, 7, 3]],\n",
       " \n",
       "         [[5, 4, 3],\n",
       "          [1, 6, 0],\n",
       "          [7, 8, 2]],\n",
       " \n",
       "         [[5, 3, 8],\n",
       "          [1, 6, 0],\n",
       "          [7, 4, 2]],\n",
       " \n",
       "         [[2, 5, 0],\n",
       "          [1, 7, 4],\n",
       "          [3, 6, 8]]]),\n",
       " tensor([3, 1, 2, 1, 2, 3, 4, 3, 3, 1, 3, 1, 2, 2, 2, 4]))"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b, m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2, 4, 3],\n",
       "         [8, 7, 1],\n",
       "         [6, 0, 5]],\n",
       "\n",
       "        [[0, 3, 2],\n",
       "         [7, 5, 8],\n",
       "         [6, 1, 4]],\n",
       "\n",
       "        [[0, 7, 3],\n",
       "         [4, 8, 5],\n",
       "         [2, 6, 1]],\n",
       "\n",
       "        [[5, 6, 1],\n",
       "         [8, 3, 2],\n",
       "         [7, 0, 4]],\n",
       "\n",
       "        [[5, 3, 0],\n",
       "         [6, 7, 1],\n",
       "         [2, 4, 8]],\n",
       "\n",
       "        [[3, 7, 2],\n",
       "         [6, 1, 8],\n",
       "         [5, 4, 0]],\n",
       "\n",
       "        [[1, 4, 3],\n",
       "         [8, 5, 0],\n",
       "         [6, 7, 2]],\n",
       "\n",
       "        [[4, 2, 7],\n",
       "         [8, 6, 3],\n",
       "         [0, 1, 5]],\n",
       "\n",
       "        [[1, 2, 5],\n",
       "         [4, 3, 7],\n",
       "         [0, 6, 8]],\n",
       "\n",
       "        [[3, 0, 1],\n",
       "         [5, 4, 2],\n",
       "         [6, 8, 7]],\n",
       "\n",
       "        [[4, 5, 1],\n",
       "         [2, 3, 8],\n",
       "         [6, 7, 0]],\n",
       "\n",
       "        [[3, 1, 4],\n",
       "         [6, 5, 7],\n",
       "         [0, 8, 2]],\n",
       "\n",
       "        [[6, 0, 2],\n",
       "         [5, 1, 8],\n",
       "         [4, 7, 3]],\n",
       "\n",
       "        [[5, 4, 0],\n",
       "         [1, 6, 3],\n",
       "         [7, 8, 2]],\n",
       "\n",
       "        [[5, 3, 0],\n",
       "         [1, 6, 8],\n",
       "         [7, 4, 2]],\n",
       "\n",
       "        [[2, 5, 0],\n",
       "         [1, 7, 4],\n",
       "         [3, 6, 8]]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "move(b,m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'g' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m b[\u001b[39m1\u001b[39m],g[\u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'g' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "b[1],g[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_moves, last_logits = sample_move(b, l, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6787, 0.2406, 0.4327, 0.5036, 0.1199, 0.5295, 0.1566, 0.1077, 0.4292,\n",
       "        0.4017, 0.1154, 0.2412, 0.3354, 0.3584, 0.0835, 0.2243],\n",
       "       grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 2, 2, 2, 1, 3, 4, 4, 2, 3, 1, 1, 3, 2, 1, 2])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_moves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3, 3, 2, 1, 3, 3, 4, 2, 5, 5, 3, 4, 5, 5, 3, 3]),\n",
       " tensor([[0.0000, 0.2310, 0.5036, 0.0000, 0.2654, 0.0000],\n",
       "         [0.0000, 0.2310, 0.5036, 0.0000, 0.2654, 0.0000],\n",
       "         [0.0000, 0.1742, 0.4327, 0.0000, 0.0000, 0.3931],\n",
       "         [0.0000, 0.0928, 0.2406, 0.5572, 0.1094, 0.0000],\n",
       "         [0.0000, 0.2310, 0.5036, 0.0000, 0.2654, 0.0000],\n",
       "         [0.0000, 0.2310, 0.5036, 0.0000, 0.2654, 0.0000],\n",
       "         [0.0000, 0.1199, 0.0000, 0.5459, 0.1338, 0.2004],\n",
       "         [0.0000, 0.1742, 0.4327, 0.0000, 0.0000, 0.3931],\n",
       "         [0.0000, 0.1117, 0.2370, 0.5295, 0.1218, 0.0000],\n",
       "         [0.0000, 0.1117, 0.2370, 0.5295, 0.1218, 0.0000],\n",
       "         [0.0000, 0.2310, 0.5036, 0.0000, 0.2654, 0.0000],\n",
       "         [0.0000, 0.1199, 0.0000, 0.5459, 0.1338, 0.2004],\n",
       "         [0.0000, 0.1117, 0.2370, 0.5295, 0.1218, 0.0000],\n",
       "         [0.0000, 0.1117, 0.2370, 0.5295, 0.1218, 0.0000],\n",
       "         [0.0000, 0.2310, 0.5036, 0.0000, 0.2654, 0.0000],\n",
       "         [0.0000, 0.2310, 0.5036, 0.0000, 0.2654, 0.0000]],\n",
       "        grad_fn=<IndexBackward0>))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_moves, last_logits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
